{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DEBUG: araBERT (initial Demo - TF) .ipynb","provenance":[{"file_id":"19qhXF6-SRnjASgMOvQidEDUMkd0XqWAK","timestamp":1628008529767},{"file_id":"1KSy89fAkWt6EGfnFQElDjXrBror9lIZh","timestamp":1627671114406}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Kx5r8J6WAwC_"},"source":["#Mount Drive"]},{"cell_type":"code","metadata":{"id":"TRoTl402AsZ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628370550772,"user_tz":-60,"elapsed":15542,"user":{"displayName":"Evan Chua","photoUrl":"","userId":"04916876280285704718"}},"outputId":"7608feee-1dd6-41bb-a6b2-5fd281ae8886"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jenrhPyzE7ix"},"source":["!mkdir arabert\n","!cp -r \"/content/drive/MyDrive/finetuned_model\" ./\n","!cp -a \"/content/drive/MyDrive/bert-base-arabertv2/.\" ./arabert"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shok9wHKyI8L"},"source":["#Installing Java and pyarabic for Farasa\n","\n","To do Farasa segmenting you will need FarasaSegmenter.jar in the same directory as the preprocess.py file \n","\n","(you can get the Farasa segmenter from http://qatsdemo.cloudapp.net/farasa/register.html)"]},{"cell_type":"code","metadata":{"id":"CFNI5kkB7VQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628370994338,"user_tz":-60,"elapsed":52717,"user":{"displayName":"Evan Chua","photoUrl":"","userId":"04916876280285704718"}},"outputId":"c1b80820-526f-483b-a420-9f770013b15b"},"source":["!pip install py4j\n","!pip install pyarabic\n","!pip install tensorflow==1.13.2\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting py4j\n","  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n","\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 35.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20 kB 38.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 43.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 122 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 133 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 184 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 194 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 198 kB 12.0 MB/s \n","\u001b[?25hInstalling collected packages: py4j\n","Successfully installed py4j-0.10.9.2\n","Collecting pyarabic\n","  Downloading PyArabic-0.6.11-py3-none-any.whl (126 kB)\n","\u001b[K     |████████████████████████████████| 126 kB 14.1 MB/s \n","\u001b[?25hInstalling collected packages: pyarabic\n","Successfully installed pyarabic-0.6.11\n","Collecting tensorflow==1.13.2\n","  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n","\u001b[K     |████████████████████████████████| 92.7 MB 15 kB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.4.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n","Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n","  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n","\u001b[K     |████████████████████████████████| 367 kB 60.3 MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.34.1)\n","Collecting tensorboard<1.14.0,>=1.13.0\n","  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 65.6 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.12.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.36.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.6.1)\n","Collecting mock>=2.0.0\n","  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.5.0)\n","Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n","Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n","Collecting transformers\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 14.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 61.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 66.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 54.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nxf11ClKpMRn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628371004800,"user_tz":-60,"elapsed":10475,"user":{"displayName":"Evan Chua","photoUrl":"","userId":"04916876280285704718"}},"outputId":"9f67f03e-3aa2-4784-ef84-2297461b7aa2"},"source":["#install java on colab (needed for Farasa)\n","import os       \n","def install_java():\n","  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n","  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n","  !java -version       #check java version\n","install_java()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["openjdk version \"11.0.11\" 2021-04-20\n","OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n","OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s7005rJmvsnB"},"source":["#This command is usefull when the java runtime hangs after a runtime restart (colab issue)\n","!pkill \"java\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftnZYmKXrUIa"},"source":["#Clone the BERT repo that is compatible with our model\n","\n","The cloned repo is made compatible with the vocab token that that have \"\\[ \\]\" (for the link \\[رابط\\], for twitter handles \\[مستخدم\\], for emails \\[بريد\\] and for the \"+\" in the Farasa segmenter ex: \"الدراسات\"-->\"\\[ال+, دراس ,+ات\\]\"\n","\n","The preprocess file that we used is included in the araBERT repository,"]},{"cell_type":"code","metadata":{"id":"ufHaEOIoyOfe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628371006515,"user_tz":-60,"elapsed":1728,"user":{"displayName":"Evan Chua","photoUrl":"","userId":"04916876280285704718"}},"outputId":"4862c5d4-ff6e-45f8-9989-df519c1a2a24"},"source":["!git clone https://github.com/WissamAntoun/bert #this implementation also has a compatible tokenizer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'bert'...\n","remote: Enumerating objects: 415, done.\u001b[K\n","remote: Total 415 (delta 0), reused 0 (delta 0), pack-reused 415\u001b[K\n","Receiving objects: 100% (415/415), 447.75 KiB | 13.17 MiB/s, done.\n","Resolving deltas: 100% (232/232), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LNRkHGtcGNkH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628371006516,"user_tz":-60,"elapsed":9,"user":{"displayName":"Evan Chua","photoUrl":"","userId":"04916876280285704718"}},"outputId":"4c6a4ac2-55c9-4914-c29b-c4d1714d0ede"},"source":["!cp \"../content/drive/MyDrive/FarasaSegmenterJar.jar\" ./bert\n","#!mv ./FarasaSegmenterJar.jar ./bert"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cp: cannot stat '../content/drive/MyDrive/FarasaSegmenterJar.jar': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EFgvI-blyyY-","colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"status":"error","timestamp":1628371008482,"user_tz":-60,"elapsed":1970,"user":{"displayName":"Evan Chua","photoUrl":"","userId":"04916876280285704718"}},"outputId":"2b942e8a-dd9f-44b6-b2bb-da057575733b"},"source":["import tensorflow as tf\n","from bert import tokenization\n","from bert.preprocess_arabert import preprocess\n","\n","from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForMaskedLM\n","model_name = \"aubmindlab/bert-base-arabertv2\"\n","arabert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n","arabert_model = AutoModel.from_pretrained(model_name)\n","#arabert_prep = ArabertPreprocessor(model_name=model_name)\n","ara_config = AutoConfig.from_pretrained(model_name)\n","#arabert_MaskedLMmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n","\n","#Mount your drive folder and configure the path to the araBERT folder\n","ARABERT_PATH = \"./arabert\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-808307905ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_arabert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/bert/preprocess_arabert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaGateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./bert/FarasaSegmenterJar.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfarasa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfarasa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFarasa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"]}]},{"cell_type":"code","metadata":{"id":"-kF8X1LgzDqY"},"source":["#test BERT tokenizer\n","#!cp \"../content/drive/MyDrive/BertLADO/vocab.txt\" ./arabert\n","bert_tokenizer = tokenization.FullTokenizer(ARABERT_PATH+\"/vocab.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkfeIGpO0NkP"},"source":["text = \" @arabert https://arabert.com الدراسات النظرية للتصميم الحديث\"\n","text_prep = preprocess(text)\n","print(text_prep)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"riK3zuM60Kpq"},"source":["bert_tokenizer.tokenize(text_prep)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ka_J4XCy6xyY"},"source":["##Tensorflow Training"]},{"cell_type":"markdown","metadata":{"id":"TvsyptgP7Vz_"},"source":["**ENABLE GPU RUNTIME if your files are on drive or colab local drive!!!**\n","\n","Test Sentiment Analysis score on a dataset like the AJGT\n","\n","K. M. Alomari, H. M. ElSherif, and K. Shaalan, “Arabic tweets sentimental analysis using machine learning,” in Proceedings of the International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, pp. 602–610, Montreal, Canada, June 2017."]},{"cell_type":"code","metadata":{"id":"q8vC_Fnd6xNv"},"source":["import os\n","import sys\n","import json\n","import nltk\n","import random\n","import logging\n","import tensorflow as tf\n","import pandas as pd\n","\n","from glob import glob\n","from tensorflow.keras.utils import Progbar\n","from tqdm import tqdm\n","sys.path.append(\"bert\")\n","\n","import bert\n","from bert import modeling, optimization, tokenization\n","from bert.run_classifier import input_fn_builder, model_fn_builder\n","\n","from sklearn.model_selection import train_test_split\n","  \n","# configure logging\n","log = logging.getLogger('tensorflow')\n","log.setLevel(logging.INFO)\n","\n","# create formatter and add it to the handlers\n","formatter = logging.Formatter('%(asctime)s :  %(message)s')\n","sh = logging.StreamHandler()\n","sh.setLevel(logging.INFO)\n","sh.setFormatter(formatter)\n","log.handlers = [sh]\n","\n","if 'COLAB_TPU_ADDR' in os.environ:\n","  log.info(\"Using TPU runtime\")\n","  USE_TPU = True\n","  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","\n","  with tf.Session(TPU_ADDRESS) as session:\n","    log.info('TPU address is ' + TPU_ADDRESS)\n","    # Upload credentials to TPU.\n","    with open('/content/drive/MyDrive/bertlado-4dd33c70e8d3.json', 'r') as f:\n","      auth_info = json.load(f)\n","    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","    \n","else:\n","  log.warning('Not connected to TPU runtime')\n","  USE_TPU = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RERzE9788SkN"},"source":["df_AJGT = pd.read_excel('./bert/AJGT.xlsx',header=0)\n","\n","DATA_COLUMN = 'text'\n","LABEL_COLUMN = 'label'\n","\n","df_AJGT = df_AJGT[['Feed', 'Sentiment']]\n","df_AJGT.columns = [DATA_COLUMN, LABEL_COLUMN]\n","\n","df_AJGT['text'] = df_AJGT['text'].apply(lambda x: preprocess(x,True))\n","\n","train_AJGT, test_AJGT = train_test_split(df_AJGT, test_size=0.2,random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODrRSrYL78KM"},"source":["# Input data pipeline config\n","TRAIN_BATCH_SIZE = 32 #@param {type:\"integer\"} #You can probably \n","                                              #increase when using TPUS\n","MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"} #512 if running on TPU\n","\n","# Training procedure config\n","EVAL_BATCH_SIZE = 64 \n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 6 #@param {type:\"integer\"}\n","WARMUP_PROPORTION = 0.1 #@param {type:\"number\"}\n","NUM_TPU_CORES = 8\n","PREDICT_BATCH_SIZE = 8\n","\n","CONFIG_FILE = os.path.join(ARABERT_PATH, \"tf1_model/tf-base-arabertv2/config.json\")\n","INIT_CHECKPOINT = os.path.join(ARABERT_PATH,\"tf1_model/tf-base-arabertv2/model.ckpt\")\n","\n","!cp '/content/drive/MyDrive/finetuned_model' ./\n","OUTPUT_DIR_PER_MODEL = \"./finetuned_model\"\n","bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n","\n","log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n","\n","print(\"ARABERT_PATH: \"+ARABERT_PATH)\n","print(\"CONFIG_FILE: \"+CONFIG_FILE)\n","print(\"INIT_CHECKPOINT: \"+INIT_CHECKPOINT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPIM4K2a80l3"},"source":["train_InputExamples = train_AJGT.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                    text_a = x[\"text\"], \n","                                                                    text_b = None, \n","                                                                    label = x[\"label\"]), axis = 1)\n","\n","test_InputExamples = test_AJGT.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                    text_a = x[\"text\"], \n","                                                                    text_b = None, \n","                                                                    label = x[\"label\"]), axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kReBtJo89p3K"},"source":["labels = list(df_AJGT.label.unique())\n","print(labels)\n","\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, labels, MAX_SEQ_LENGTH, bert_tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, labels, MAX_SEQ_LENGTH, bert_tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfDMcI2l95Y9"},"source":["num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","num_steps_per_epoch = int(len(train_features) / TRAIN_BATCH_SIZE)\n","\n","print(\"num train steps: {}\".format(num_train_steps))\n","print(\"num warmup steps: {}\".format(num_warmup_steps))\n","print(\"num_steps_per_epoch: {}\".format(num_steps_per_epoch))\n","\n","model_fn = model_fn_builder(\n","  bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n","  #bert_config=ara_config,\n","  num_labels=2,\n","  init_checkpoint=INIT_CHECKPOINT,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=USE_TPU,\n","  use_one_hot_embeddings=USE_TPU\n",")\n","\n","tpu_cluster_resolver = None\n","if USE_TPU:\n","  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","\n","run_config = tf.contrib.tpu.RunConfig(\n","    cluster=tpu_cluster_resolver,\n","    model_dir=OUTPUT_DIR_PER_MODEL,\n","    save_checkpoints_steps=num_steps_per_epoch,\n","    keep_checkpoint_max=0,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=num_steps_per_epoch,\n","        num_shards=NUM_TPU_CORES,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=USE_TPU,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)\n","  \n","train_input_fn = input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=USE_TPU)\n","\n","test_input_fn = input_fn_builder(\n","  features=test_features,\n","  seq_length=MAX_SEQ_LENGTH,\n","  is_training=False,\n","  drop_remainder=USE_TPU)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EaIt17wo-8dU"},"source":["##Train the model"]},{"cell_type":"code","metadata":{"id":"gxZTFBMgOhbU"},"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFAHCPW--tdY"},"source":["print(f'Beginning Training!')\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GqBb9JI--pg"},"source":["##Evaluate the model on all saved checkpoint files"]},{"cell_type":"code","metadata":{"id":"YE2vyARj-2jH"},"source":["print(f'Beginning Evaluation!')\n","eval_model_files = tf.gfile.Glob(os.path.join(OUTPUT_DIR_PER_MODEL,'*index'))\n","\n","for eval_checkpoint in tqdm(sorted(eval_model_files,key=lambda x: int(x[0:-6].split('-')[-1]))):\n","  result = estimator.evaluate(input_fn=test_input_fn, steps=int(len(test_features)/EVAL_BATCH_SIZE),checkpoint_path=eval_checkpoint[0:-6])\n","  tf.logging.info(\"***** Eval results *****\")\n","  for key in sorted(result.keys()):\n","    tf.logging.info(\"  %s = %s\", key, str(result[key]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2rGGQj6BUfal"},"source":["##Results\n","\n","araBERT achieved >93 acc on AJGT compared to 84 for mBERT (Tested prev, you can also try it using tf_hub scripts)\n","\n","we think that araBERT can get better score with more data cleaning and preprocessing.\n","\n","It also shows shows that it can adapt well for dialectal data (which is the most comon)"]}]}